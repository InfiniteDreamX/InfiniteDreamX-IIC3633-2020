# Performance of Recommender Algorithms on Top-N Recommendation Tasks
Este paper se centra principalmente en el problema del Top-N, que es básicamente recomendar los N items que más le podrían gustar al usuario. En este problema, no es relevante el rating que se predice, sino que lo importante es que, de todos los items que hayan, se logre hacer un conjunto de items que tenga recomendaciones relevantes y novedosas para el usuario. Esto último choca con el acercamiento más clásico en donde se busca reducir el error RSME, y por lo tanto el paper presenta métodos que, si bien según la métrica del RSME no son buenos (o mejor dicho directamente no se pueden evaluar, ya que no entregan un rating para comparar), presentan mejor performance en la tarea del Top-N que los métodos estado del arte que se centran en minimizar el RSME, todo esto evaluado con métricas de precisión como el Recall y Precision.

Me gustó la forma en que decidieron hacer el testeo de los modelos, ya que para el problema de Top-N, creo que es bastante adecuado que se testee con un conjunto de 1000 items random más el item del set de testing que posee rating máximo por el usuario. Esto es porque efectivamente en la práctica uno va a querer encontrar las mejores recomendaciones para un usuario dentro de un conjunto potencialmente muy grande de items. Por eso mismo, creo que es bastante válido el supuesto de que la gran mayoría de esos 1000 items no son relevantes, ya que los casos negativos de este supuesto es sólamente cuando efectivamente en el conjunto hayan N items mejores para el usuario que el item del dataset de test, y este caso creo que es bastante improbable. Por el otro lado, podemos estar bastante seguros de que si el recomendador logra poner el item del set de test dentro de los Top N, entonces esta realizando una buena recomendación, independiente de el lugar donde lo haya puesto.

¿ Porqué decidieron utilizar una distinta cantidad de factores latentes en el PureSVD para cada uno de los datasets ? Considero que es una decisión bastante interesante que lamentablemente no se dieron el tiempo para explicar en el paper. Imagino que posiblemente se haya hecho porque, tal como vimos en papers pasados, más factores latentes es en general mejor, y como el dataset de Netflix es bastante más grande que el de MovieLens (100M ratings vs 1M ratings respectivamente) entonces había data suficiente para "soportar" una mayor cantidad de factores latentes, o quizás simplemente decidieron esto porque la data del de netflix es más compleja y necesita mayor cantidad de factores para ser descrita. Sea cual sea el caso, hubiera sido interesante que explicaran esta decisión en el paper.

Por último, me parece muy interesante que el método CorNgbr haya mejorado con el dataset de Netflix al momento de evaluar con la long tail (siendo de hecho uno de los mejores), y que por el contrario en el dataset de MovieLens en la long tail haya sido de los peores. Es entendible que los autores no hayan discutido más al respecto acerca de esto ya que no es el tema del paper, pero es un hallazgo a mi parecer interesante que puede ser por múltiples motivos, como que quizás la implementación CorNgbr en la que se basaron haya sido creada justamente para el dataset de Netflix para poder hacer predicciones no triviales (de la long tail), o quizás sucedió debido a que la cantidad de datos del dataset de Netflix permitió esto, y si hubiera habido la misma cantidad de datos en el de MovieLens hubiese sucedido lo mismo. 
