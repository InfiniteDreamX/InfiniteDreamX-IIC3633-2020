# Multi-Armed Recommender System Bandit Ensembles
Este paper busca evaluar sistemas recomendadores ensamblados (que son una mezcla de varios algoritmos de recomendacion) en donde se utiliza la técnica de multi-armed bandit para elegir el algoritmo a utilizar para generar la recomendación. Lo interesante es que los evalua en una situación más realista en donde la mayoría del feedback que recibe el sistema es a partir de las recomendaciones que genera, haciendo varias recomendaciones en las distintas epocas para así ir mejorando gracias a que los usuarios usen cada vez más el sistema.

En primer lugar, me parece muy interesante el caso de evaluación que atacan en este paper, dado que es un enfoque mucho más realista en donde se tiene que lidiar con no tener un dataset grande desde el inicio, sino que este se va construyendo a medida que los usuarios interactuan con el modelo. Este enfoque lo veo particularmente útil para sitios nuevos que esten implementando un sistema recomendador, por lo que no tienen tantos datos para entrenar los modelos pero aún así necesitan tener un recomendador. Por otro lado, este enfoque quizás no sea tan relevante para empresas que ya posean grandes cantidades de datos, pero aún en estos casos creo que este enfoque, por lo menos, ayuda a atacar el problema del  cold start, por lo que sigue siendo relevante investigarlo

Siguiendo lo anterior, encuentro que la forma de simular esto en un entorno offline fue bastante ingeniosa, dado que rompe con el típico paradigma de tener un train set inicial con la mayoria de los datos (80% o más), y en vez de esto el primer train set es una porción muy pequeña de los datos (5%) con los cuales entrenan en una primera instancia a los sistemas. Luego, solamente agregan nuevos datos al train set si es que fueron predecidos por el sistema y hay feedback de dicho elemento para ese determinado usuario en el test set, y de esta manera el test set simula como si fuese feedback de los usuarios luego de haber visto la recomendacion hecha por el sistema. Este formato es bastante interesante y considero que es bastante acertado para simular una situación más realista de recomendacion, ya que los datos solamente son presentados si es que el sistema los recomendo, y si no los recomienda entonces nunca podrá ver dicho item.

Una cosa que no me quedo clara de porque lo hicieron es la binarización de los datos. En el dataset original los datos poseen rankings del 1 al 5, pero los autores deciden dividirlos en solamente 0 o 1. Aunque no es una decisión mala ni nada, no se explica en el paper el motivo de esto, y tampoco tiene demasiado sentido teniendo en cuenta de que no nos estamos enfrentando a feedback implicito (ya que en esos casos si se suele binarizar) sino que el feedback que dan los usuarios es explicito. Por esto mismo, no le veo la ganancia de reducir los datos a 0 y 1, ya que claramente una escala del 1 al 5 es mucho más descriptiva para saber cuanto le gusto al usuario y serviría más para poder evaluar que tan buenas estan siendo las recomendaciones. A mi parecer, hubiera sido mejor mantener el feedback del 1 al 5, y ponderar la recompensa dependiendo de que tan alto es el feedback.

Una cosa corta que me gustaría comentar es que creo que fue una buena idea comparar con random tambien, simplemente por el hecho de que ayuda a visualizar que los algoritmos más malos igual estan funcionando, ya que son muy supierores a random de igual forma. Sin esta comparación, uno podría pensar que los algoritmos más malor del gráfico realmente funcionan muy mal, cuando no es necesariamente el caso.

Finalmente, creo que la elección de algoritmos para formar el ensamblado no estuvo del todo bien. En específico, creo que fue una mala idea poner 2 algoritmos de filtrado colaborativo, ya que si bien puede que funcionen distinto y tengan performances distintas, al fin y al cabo utilizan el mismo criterio para generar las recomendaciones, por lo que realmente el sistema no esta ganando nuevas perspectivas para recomendar ni esta logrando cubrir situaciones nuevas, como si lo hace por ejemplo el algoritmo most popular, que cubre el inicio cuando aún no se poseen muchos datos (cold start). En vez de esto, creo que hubiera sido mucho mejor opción dejar 1 sólo algoritmo de filtrado colaborativo y el otro reemplazarlo por otro tipo de algoritmo, como podría ser uno basado en contenido. De esta manera, cada algoritmo del sistema utilizaría un criterio distinto para recomendar, y así lograrían tener mayor variedad de recomendaciones y cubrir más casos que los de filtrado colaborativo.
